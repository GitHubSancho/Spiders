### 分布式爬虫
#### 对多台计算机搭建联合爬取程序，提升爬取数据效率
#### 安装scrapy-redis组件
#### 实现流程：
- 创建一个工程文件夹
- 创建一个基于CrawlSpider的爬虫文件  
` scrapy genspider -t crawl spiderName www.起始域名.com `
- 修改爬虫文件：
```	
1.spider文件导入包 
	from scrapy_redis.spiders import RedisCrawlSpider  
2.将start_urls和allowed_domains变量注释  
3.增加redis_key= 'Name'，Name=可以被共享的调度器队列的名称  
4.写入解析条件  
5.封装到Item，Item中增加将获取的属性  
6.将爬虫的父类改成RedisCrawlSpider  
7.修改settings.py文件，robot规则、UA等  
8.开启共享管道  
	ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline':400}  
9.指定调度器  
	DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'  
10.使用scrapy-redis组件自己的调度器  
	SCHEDULER = "scrapy_redis.scheduler.Scheduler"  
11.配置调度器是否需要持久化   
	SCHEDULER_PERSIST = True  
12.指定redis服务器
	REDIS_HOST = 'redis服务的ip地址' #服务端写127.0.0.1  
	REDIS_PROT = '端口号'
```

- 配置redis  
```	
1.下载对应系统的配置文件  
2.打开配置文件并修改  
	删除bind 127.0.0.1  
	#关闭保护模式  
	protected-mode no  
3.启动redis服务  
	redis-server 配置文件名  
	#启动客户端  
	./redis-cli 
```
- 执行工程(所有机群)  
```
scrapy runspider spiderName.py  
#向调度器队列中放入一个起始url  
lpush 共享的调度器名称 起始url  
#爬取的数据存储在redis的proName:items这个数据结构中
```
