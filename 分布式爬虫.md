### 分布式爬虫
#### 对多台计算机搭建联合爬取程序，提升爬取数据效率
#### 安装scrapy-redis组件
#### 实现流程：
<li>创建一个工程文件夹
<li>创建一个基于CrawlSpider的爬虫文件
	scrapy genspider -t crawl spiderName www.起始域名.com
<li>修改爬虫文件：
	#spider文件导入包  
	from scrapy_redis.spiders import RedisCrawlSpider  
	#将start_urls和allowed_domains变量注释  
	#增加redis_key= 'Name'，Name=可以被共享的调度器队列的名称  
	#写入解析条件  
	#封装到Item，Item中增加将获取的属性  
	#将爬虫的父类改成RedisCrawlSpider  
	#修改settings.py文件，robot规则、UA等  
	#开启共享管道  
	ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline':400}  
	#指定调度器  
	DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'  
	#使用scrapy-redis组件自己的调度器  
	SCHEDULER = "scrapy_redis.scheduler.Scheduler"  
	#配置调度器是否需要持久化   
	SCHEDULER_PERSIST = True  
	#指定redis服务器
	REDIS_HOST = 'redis服务的ip地址' #服务端写127.0.0.1  
	REDIS_PROT = '端口号'
<li>配置redis
	#下载对应系统的配置文件  
	#打开配置文件并修改  
	删除bind 127.0.0.1  
	#关闭保护模式  
	protected-mode no  
	#启动redis服务  
	redis-server 配置文件名  
	#启动客户端  
	./redis-cli  
<li>执行工程(所有机群)
	scrapy runspider spiderName.py  
	#向调度器队列中放入一个起始url  
	lpush 共享的调度器名称 起始url  
	#爬取的数据存储在redis的proName:items这个数据结构中
